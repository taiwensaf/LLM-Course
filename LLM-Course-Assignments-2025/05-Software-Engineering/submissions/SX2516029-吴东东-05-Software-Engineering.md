项目的独立仓库地址：https://github.com/taiwensaf/wddCodeAgent
# PR #02: 测试与调试逻辑加强、接口契约升级

> **提交日期**: 2025年12月22日  
> **项目名称**: wddCodeAgent - AI 驱动的智能代码生成系统

---

## 1. 选题背景

随着大语言模型在代码理解和生成方面的能力显著提升，软件开发流程正在经历革命性变革。然而，现有工具往往只关注单一任务（如代码补全），缺乏从需求到测试的完整自动化流程。

**本项目旨在解决**：
- 从自然语言需求到可运行代码的鸿沟
- 代码生成后的测试和调试自动化
- 多模块项目的智能任务拆解与代码组织

---

## 2. 项目目标

构建一个端到端的 AI 软件开发助手，实现：

✅ **代码生成**: 从自然语言需求自动生成完整、可运行的代码  
✅ **智能规划**: 复杂需求自动拆解为多个独立任务  
✅ **自动测试**: 为生成代码自动创建并执行单元测试  
✅ **错误修复**: 基于测试失败信息智能调试和修复代码  
✅ **可复现性**: 提供完整的实验流程和评估指标

---

## 3. 技术路线

### 核心技术栈
- **大模型**: Qwen2.5-Coder (7B) Ollama 本地部署
- **编程语言**: Python 3.10+
- **测试框架**: pytest
- **LLM 接口**: Ollama (本地) / OpenAI (云端可选。未实现)

### 系统架构
```
用户需求
    ↓
[Planner] 需求分析与任务拆解
    ↓
[Coder] 多任务代码生成与汇总
    ↓
[Tester] 自动化测试生成与执行
    ↓
[Debugger] 失败时智能调试修复 (最多3轮（可选）)
    ↓
完整可运行代码 + 测试报告
```

### 核心模块设计
- `agent_loop.py`: 主工作流控制器
- `planner.py`: 需求规划（LLM + Prompt Engineering）
- `coder.py`: 代码生成与多任务汇总
- `tester.py`: 测试生成与执行（pytest集成）
- `debugger.py`: 基于错误信息的代码修复
- `llm_client.py`: 统一 LLM 接口（支持 Ollama/OpenAI）
---

## 4. 已实现功能

### ✅ 第一阶段功能（PR #01 已完成）

- [x] **智能需求规划**
  - 自动区分单任务/多任务场景
  - 项目命名规范化（snake_case）
  
- [x] **多任务代码生成**
  - 遍历所有规划任务并生成代码
  - 增量生成（后续任务可见前面代码）
  - 智能去重（避免重复函数/类）
  
- [x] **自动化测试**
  - pytest 测试用例自动生成
  - 相对路径导入自动修复
  - 详细测试统计（通过/失败/耗时）
  
- [x] **智能调试**
  - 基于测试失败的自动修复（最多3轮）
  - 调试历史追踪
  
- [x] **代码质量保障**
  - 文件命名规范化（snake_case）
  - PEP8 代码规范（由 LLM 生成时保证）

### ✅ 第二阶段新增（本PR）

- [x] **测试逻辑加强**
  - 统一测试工件约定：生成的测试文件、临时工作目录、依赖声明写入同一元数据描述，便于后续可复现
  - 默认启用精简 pytest 配置（`-q --maxfail=1`），并输出覆盖率摘要与失败用例最小复现脚本
  - 针对多任务场景，按子任务分组生成测试套件并自动修正相对导入与路径

- [x] **调试逻辑加强**
  - 调试循环接口升级：每轮调试带失败摘要、建议修复片段、补丁候选与应用结果，便于快速审阅
  - 支持“仅重跑失败用例”模式，缩短迭代时间；调试轮次可配置（默认最多3轮）
  - 将调试前/后文件 diff 与测试日志落盘，方便回溯与 PR 附件

- [x] **接口契约升级**
  - planner / coder / tester / debugger 之间的输入输出 schema 对齐，新增状态码、错误码与文件清单，便于扩展前后端/多语言
  - CLI 输出增加关键节点事件流（规划、生成、测试、调试）与工件路径索引，方便批量评估


### 下一步工作（2周内）
1. 完成多文件项目结构与包级依赖自动安装
2. 引入模型切换与端点自动选择（OpenAI / DeepSeek / 本地）
3. ~~跑通 HumanEval + MBPP 小集评测并落盘报告~~ ✅ **已完成**
4. 补充可视化调试时间线（CLI/Web）与人工确认回路

---

## 📊 PR #03: HumanEval 评估完成

> **提交日期**: 2026年1月4日  
> **评估任务**: HumanEval 代码生成能力评估

### ✅ 评估结果

已完成 HumanEval 基准测试，评估结果如下：

| 指标 | 数值 |
|------|------|
| 评估问题数 | 164 |
| 通过问题数 | 140 |
| **通过率** | **85.4%** |
| 样本文件 | `results\humaneval_samples.jsonl` |

### 📈 评估说明

- **测试集**: HumanEval 标准测试集（164道编程题）
- **评估模型**: Qwen2.5-Coder (7B)
- **评估方式**: 端到端代码生成 + 自动化测试验证
- **通过标准**: 生成代码通过官方测试用例

### 🎯 结果分析

本次评估通过率达到 **85.4%**，表明：
- ✅ 系统在常见算法和数据结构问题上表现稳定
- ✅ 代码生成质量达到实用级别
- ✅ 自动化测试与调试机制有效

### 📁 评估工件

- 详细评估日志：`results\humaneval_samples.jsonl`
- 包含每道题的生成代码、测试结果和执行时间

---

## 5. 使用示例

### 安装与运行
```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 启动 Ollama（本地）
ollama pull qwen2.5-coder:7b

# 3. 运行示例
python cli.py "实现一个计算器函数" --model qwen2.5-coder:7b --plan
```

### 输出示例


---

## 8. 项目结构

```
wddCodeAgent/
├── cli.py                  # 命令行入口
├── requirements.txt        # 依赖列表
├── PROJECT_STATUS.md       # 项目状态文档
├── PR_01_PROJECT_INIT.md   # 本文档
│
├── agent/                  # 核心模块
│   ├── agent_loop.py      # 主工作流
│   ├── planner.py         # 需求规划
│   ├── coder.py           # 代码生成
│   ├── tester.py          # 测试执行
│   ├── debugger.py        # 代码调试
│   ├── llm_client.py      # LLM 接口
│   └── prompt_manager.py  # Prompt 管理
│
├── prompt/                 # Prompt 模板
│   ├── planner.txt
│   ├── coder.txt
│   ├── tester.txt
│   └── debugger.txt
│
└── results/                # 输出结果
    ├── generated_code/     # 生成的代码
    └── tests/              # 生成的测试
```


---

**审核清单**
- [x] 代码已通过本地测试
- [x] 文档完整且清晰
- [x] 符合项目代码规范
- [x] 可复现实验流程

**下一个PR预告**: 多文件项目结构 + 依赖自动管理
